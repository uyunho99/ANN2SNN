{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Network_ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        input size : [bs, 3, 32, 32]\n",
    "        \"\"\"\n",
    "        super(Network_ANN, self).__init__()\n",
    "        self.dropout_rate = 0.5\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, bias=False)   # [bs, 64, 32, 32]\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=0.01)\n",
    "        self.HalfRect1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        # self.subsample1 = nn.AvgPool2d(2, 2, 0)         # [bs, 64, 16, 16]\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False)  # [bs, 64, 32, 32]\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=0.01)\n",
    "        self.HalfRect2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.subsample2 = nn.AvgPool2d(2, 2, 0)         # [bs, 128, 16, 16]\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, bias=False)  # [bs, 128, 16, 16]\n",
    "        self.bn3 = nn.BatchNorm2d(128, momentum=0.01)\n",
    "        self.HalfRect3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "        # self.subsample3 = nn.AvgPool2d(2, 2, 0)         # [bs, 256, 4, 4]\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, bias=False)  # [bs, 128, 16, 16]\n",
    "        self.bn4 = nn.BatchNorm2d(128, momentum=0.01)\n",
    "        self.HalfRect4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(self.dropout_rate)\n",
    "        self.subsample4 = nn.AvgPool2d(2, 2, 0)         # [bs, 128, 8, 8]\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 10, bias=False)\n",
    "        self.HalfRectLast = nn.ReLU()\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        super().to(device)\n",
    "        return self\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.dropout1(self.HalfRect1(self.bn1(self.conv1(input))))\n",
    "        # x = self.subsample1(x)\n",
    "        x = self.dropout2(self.HalfRect2(self.bn2(self.conv2(x))))\n",
    "        x = self.subsample2(x)\n",
    "        x = self.dropout3(self.HalfRect3(self.bn3(self.conv3(x))))\n",
    "        # x = self.subsample3(x)\n",
    "        x = self.dropout4(self.HalfRect4(self.bn4(self.conv4(x))))\n",
    "        x = self.subsample4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.HalfRectLast(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def normalize_nn(self, train_loader):\n",
    "        conv1_weight_max = torch.max(F.relu(self.conv1.weight))\n",
    "        conv2_weight_max = torch.max(F.relu(self.conv2.weight))\n",
    "        conv3_weight_max = torch.max(F.relu(self.conv3.weight))\n",
    "        conv4_weight_max = torch.max(F.relu(self.conv4.weight))\n",
    "        fc1_weight_max = torch.max(F.relu(self.fc1.weight))\n",
    "\n",
    "        conv1_activation_max = 0.0\n",
    "        conv2_activation_max = 0.0\n",
    "        conv3_activation_max = 0.0\n",
    "        conv4_activation_max = 0.0\n",
    "        fc1_activation_max = 0.0\n",
    "\n",
    "        self.eval()\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            x = inputs.float().to(self.device)\n",
    "            \n",
    "            x = self.dropout1(self.HalfRect1(self.bn1(self.conv1(input))))\n",
    "            conv1_activation_max = max(conv1_activation_max, torch.max(x))\n",
    "            # x = self.subsample1(x)\n",
    "            \n",
    "            x = self.self.dropout2(self.HalfRect2(self.bn2(self.conv2(x))))\n",
    "            conv2_activation_max = max(conv2_activation_max, torch.max(x))\n",
    "            x = self.subsample2(x)\n",
    "            \n",
    "            x = self.dropout3(self.HalfRect3(self.bn3(self.conv3(x))))\n",
    "            conv3_activation_max = max(conv3_activation_max, torch.max(x))\n",
    "            # x = self.subsample3(x)\n",
    "\n",
    "            x = self.dropout4(self.HalfRect4(self.bn4(self.conv4(x))))\n",
    "            conv4_activation_max = max(conv4_activation_max, torch.max(x))\n",
    "            x = self.subsample4(x)\n",
    "\n",
    "            x = x.view(-1, 128 * 8 * 8)\n",
    "            x = self.HalfRectLast(self.fc1(x))\n",
    "            fc1_activation_max = max(fc1_activation_max, torch.max(x))\n",
    "        self.train()\n",
    "\n",
    "        self.factor_log = []\n",
    "        previous_factor = 1\n",
    "\n",
    "        scale_factor = max(conv1_weight_max, conv1_activation_max)\n",
    "        applied_inv_factor = (scale_factor / previous_factor).item()\n",
    "        self.conv1.weight.data = self.conv1.weight.data / applied_inv_factor\n",
    "        self.factor_log.append(1 / applied_inv_factor)\n",
    "        previous_factor = applied_inv_factor\n",
    "\n",
    "        scale_factor = max(conv2_weight_max, conv2_activation_max)\n",
    "        applied_inv_factor = (scale_factor / previous_factor).item()\n",
    "        self.conv2.weight.data = self.conv2.weight.data / applied_inv_factor\n",
    "        self.factor_log.append(1 / applied_inv_factor)\n",
    "        previous_factor = applied_inv_factor\n",
    "\n",
    "        scale_factor = max(conv3_weight_max, conv3_activation_max)\n",
    "        applied_inv_factor = (scale_factor / previous_factor).item()\n",
    "        self.conv3.weight.data = self.conv3.weight.data / applied_inv_factor\n",
    "        self.factor_log.append(1 / applied_inv_factor)\n",
    "        previous_factor = applied_inv_factor\n",
    "\n",
    "        scale_factor = max(conv4_weight_max, conv4_activation_max)\n",
    "        applied_inv_factor = (scale_factor / previous_factor).item()\n",
    "        self.conv4.weight.data = self.conv4.weight.data / applied_inv_factor\n",
    "        self.factor_log.append(1 / applied_inv_factor)\n",
    "        previous_factor = applied_inv_factor\n",
    "\n",
    "        scale_factor = max(fc1_weight_max, fc1_activation_max)\n",
    "        applied_inv_factor = (scale_factor / previous_factor).item()\n",
    "        self.fc1.weight.data = self.fc1.weight.data / applied_inv_factor\n",
    "        self.factor_log.append(1 / applied_inv_factor)\n",
    "        previous_factor = applied_inv_factor\n",
    "\n",
    "\n",
    "class Network_SNN(nn.Module):\n",
    "    def __init__(self, time_window=50, threshold=1.2, max_rate=200):\n",
    "        super(Network_SNN, self).__init__()\n",
    "        self.dropout_rate = 0.5\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1, bias=False)   # [bs, 64, 32, 32]\n",
    "        self.bn1 = nn.BatchNorm2d(64, momentum=0.01)\n",
    "        self.HalfRect1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(self.dropout_rate)\n",
    "        # self.subsample1 = nn.AvgPool2d(2, 2, 0)         # [bs, 64, 16, 16]\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, bias=False)  # [bs, 64, 32, 32]\n",
    "        self.bn2 = nn.BatchNorm2d(64, momentum=0.01)\n",
    "        self.HalfRect2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(self.dropout_rate)\n",
    "        self.subsample2 = nn.AvgPool2d(2, 2, 0)         # [bs, 128, 16, 16]\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1, bias=False)  # [bs, 128, 16, 16]\n",
    "        self.bn3 = nn.BatchNorm2d(128, momentum=0.01)\n",
    "        self.HalfRect3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(self.dropout_rate)\n",
    "        # self.subsample3 = nn.AvgPool2d(2, 2, 0)         # [bs, 256, 4, 4]\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1, bias=False)  # [bs, 128, 16, 16]\n",
    "        self.bn4 = nn.BatchNorm2d(128, momentum=0.01)\n",
    "        self.HalfRect4 = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(self.dropout_rate)\n",
    "        self.subsample4 = nn.AvgPool2d(2, 2, 0)         # [bs, 128, 8, 8]\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 10, bias=False)\n",
    "        self.HalfRectLast = nn.ReLU()\n",
    "\n",
    "        self.threshold = threshold\n",
    "        self.time_window = time_window\n",
    "        self.dt = 0.001  # second\n",
    "        self.refractory_t = 0\n",
    "        self.max_rate = max_rate\n",
    "        self.rescale_factor = 1.0 / (self.dt * self.max_rate)\n",
    "\n",
    "    def to(self, device):\n",
    "        self.device = device\n",
    "        super().to(device)\n",
    "        return self\n",
    "\n",
    "    def mem_update(self, t,  operator, mem, input_spk, leak=0, refrac_end=None):\n",
    "        # Get input impulse from incoming spikes\n",
    "        impulse = operator(input_spk)\n",
    "        # Add input to membrane potential\n",
    "        mem = mem + impulse + leak\n",
    "        # Check for spiking\n",
    "        output_spk = (mem >= self.threshold).float()\n",
    "        # Reset\n",
    "        mem = mem * (1. - output_spk)\n",
    "        # Ban updates until....\n",
    "        if refrac_end is not None:\n",
    "            refrac_end = output_spk * (t + self.refractory_t)\n",
    "        return mem, output_spk\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.size(0)\n",
    "\n",
    "        spk_input = spksum_input = torch.zeros(\n",
    "            batch_size, 3, 32, 32, device=self.device)\n",
    "        mem_post_conv1 = spk_post_conv1 = spksum_post_conv1 = torch.zeros(\n",
    "            batch_size, 64, 32, 32, device=self.device)\n",
    "        # mem_post_subsample1 = spk_post_subsample1 = spksum_post_subsample1 = torch.zeros(\n",
    "        #     batch_size, 16, 14, 14, device=self.device)\n",
    "        mem_post_conv2 = spk_post_conv2 = spksum_post_conv2 = torch.zeros(\n",
    "            batch_size, 64, 32, 32, device=self.device)\n",
    "        mem_post_subsample2 = spk_post_subsample2 = spksum_post_subsample2 = torch.zeros(\n",
    "            batch_size, 64, 16, 16, device=self.device)\n",
    "        mem_post_conv3 = spk_post_conv3 = spksum_post_conv3 = torch.zeros(\n",
    "            batch_size, 128, 16, 16, device=self.device)\n",
    "        mem_post_conv4 = spk_post_conv4 = spksum_post_conv4 = torch.zeros(\n",
    "            batch_size, 128, 16, 16, device=self.device)\n",
    "        mem_post_subsample4 = spk_post_subsample4 = spksum_post_subsample4 = torch.zeros(\n",
    "            batch_size, 128, 8, 8, device=self.device)\n",
    "        mem_post_fc1 = spk_post_fc1 = spksum_post_fc1 = torch.zeros(\n",
    "            batch_size, 10, device=self.device)\n",
    "\n",
    "        for t in range(self.time_window):\n",
    "            spk_input = (torch.rand(input.size(), device=self.device)\n",
    "                         * self.rescale_factor <= input).float()\n",
    "            spksum_input = spksum_input + spk_input\n",
    "\n",
    "            mem_post_conv1, spk_post_conv1 = self.mem_update(\n",
    "                t, self.conv1, mem_post_conv1, spk_input)\n",
    "            spksum_post_conv1 = spksum_post_conv1 + spk_post_conv1\n",
    "\n",
    "            # mem_post_subsample1, spk_post_subsample1 = self.mem_update(\n",
    "            #     t, self.subsample1, mem_post_subsample1, spk_post_conv1)\n",
    "            # spksum_post_subsample1 = spksum_post_subsample1 + spk_post_subsample1\n",
    "\n",
    "            mem_post_conv2, spk_post_conv2 = self.mem_update(\n",
    "                t, self.conv2, mem_post_conv2, spk_post_conv1)\n",
    "            spksum_post_conv2 = spksum_post_conv2 + spk_post_conv2\n",
    "\n",
    "            mem_post_subsample2, spk_post_subsample2 = self.mem_update(\n",
    "                t, self.subsample2, mem_post_subsample2, spk_post_conv2)\n",
    "            spksum_post_subsample2 = spksum_post_subsample2 + spk_post_subsample2\n",
    "\n",
    "            mem_post_conv3, spk_post_conv3 = self.mem_update(\n",
    "                t, self.conv3, mem_post_conv3, spk_post_subsample2)\n",
    "            spksum_post_conv3 = spksum_post_conv3 + spk_post_conv3\n",
    "\n",
    "            mem_post_conv4, spk_post_conv4 = self.mem_update(\n",
    "                t, self.conv4, mem_post_conv4, spk_post_conv3)\n",
    "            spksum_post_conv4 = spksum_post_conv4 + spk_post_conv4\n",
    "\n",
    "            mem_post_subsample4, spk_post_subsample4 = self.mem_update(\n",
    "                t, self.subsample4, mem_post_subsample4, spk_post_conv4)\n",
    "            spksum_post_subsample4 = spksum_post_subsample4 + spk_post_subsample4\n",
    "\n",
    "            spk_post_conv4_ = spk_post_subsample4.view(batch_size, 128 * 8 * 8)\n",
    "            mem_post_fc1, spk_post_fc1 = self.mem_update(\n",
    "                t, self.fc1, mem_post_fc1, spk_post_conv4_)\n",
    "            spksum_post_fc1 = spksum_post_fc1 + spk_post_fc1\n",
    "        outputs = spksum_post_fc1 / self.time_window\n",
    "        return outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from torch.autograd import Variable\n",
    "    features = []\n",
    "\n",
    "    def hook(self, input, output):\n",
    "        print(output.data.cpu().numpy().shape)\n",
    "        features.append(output.data.cpu().numpy())\n",
    "    net = Network_ANN()\n",
    "    net.to(torch.device(\"cpu\"))\n",
    "    net.train()\n",
    "\n",
    "    for w in net.named_parameters():\n",
    "        print(w[0])\n",
    "\n",
    "    for m in net.modules():\n",
    "        m.register_forward_hook(hook)\n",
    "\n",
    "    # (batch_size, channels, height, width)\n",
    "    y = net(Variable(torch.randn(7, 3, 32, 32)))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/50], Training Loss: 19.65545 Time elasped:8.26 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.89 s\n",
      "Epoch [2/50], Training Loss: 19.60000 Time elasped:7.28 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.05 s\n",
      "Epoch [3/50], Training Loss: 19.60000 Time elasped:7.30 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.10 s\n",
      "Epoch [4/50], Training Loss: 19.60000 Time elasped:7.31 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.19 s\n",
      "Epoch [5/50], Training Loss: 19.60000 Time elasped:7.31 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.20 s\n",
      "Epoch [6/50], Training Loss: 19.60000 Time elasped:7.31 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.20 s\n",
      "Epoch [7/50], Training Loss: 19.60000 Time elasped:7.39 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.31 s\n",
      "Epoch [8/50], Training Loss: 19.60000 Time elasped:7.24 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.26 s\n",
      "Epoch [9/50], Training Loss: 19.60000 Time elasped:7.32 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.28 s\n",
      "Epoch [10/50], Training Loss: 19.60000 Time elasped:7.32 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.22 s\n",
      "Epoch [11/50], Training Loss: 19.60000 Time elasped:7.33 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.27 s\n",
      "Epoch [12/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.27 s\n",
      "Epoch [13/50], Training Loss: 19.60000 Time elasped:7.33 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.28 s\n",
      "Epoch [14/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.33 s\n",
      "Epoch [15/50], Training Loss: 19.60000 Time elasped:7.32 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.30 s\n",
      "Epoch [16/50], Training Loss: 19.60000 Time elasped:7.33 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.38 s\n",
      "Epoch [17/50], Training Loss: 19.60000 Time elasped:7.34 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.46 s\n",
      "Epoch [18/50], Training Loss: 19.60000 Time elasped:7.37 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.48 s\n",
      "Epoch [19/50], Training Loss: 19.60000 Time elasped:7.36 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.48 s\n",
      "Epoch [20/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.34 s\n",
      "Epoch [21/50], Training Loss: 19.60000 Time elasped:7.34 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.31 s\n",
      "Epoch [22/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.31 s\n",
      "Epoch [23/50], Training Loss: 19.60000 Time elasped:7.34 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.32 s\n",
      "Epoch [24/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.35 s\n",
      "Epoch [25/50], Training Loss: 19.60000 Time elasped:7.37 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.37 s\n",
      "Epoch [26/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.35 s\n",
      "Epoch [27/50], Training Loss: 19.60000 Time elasped:7.37 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.37 s\n",
      "Epoch [28/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.32 s\n",
      "Epoch [29/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.35 s\n",
      "Epoch [30/50], Training Loss: 19.60000 Time elasped:7.33 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.28 s\n",
      "Epoch [31/50], Training Loss: 19.60000 Time elasped:7.34 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.29 s\n",
      "Epoch [32/50], Training Loss: 19.60000 Time elasped:7.35 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.33 s\n",
      "Epoch [33/50], Training Loss: 19.60000 Time elasped:7.34 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.32 s\n",
      "Epoch [34/50], Training Loss: 19.60000 Time elasped:7.47 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.47 s\n",
      "Epoch [35/50], Training Loss: 19.60000 Time elasped:7.37 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:74.73 s\n",
      "Epoch [36/50], Training Loss: 19.60000 Time elasped:9.77 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:86.17 s\n",
      "Epoch [37/50], Training Loss: 19.60000 Time elasped:11.80 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:88.22 s\n",
      "Epoch [38/50], Training Loss: 19.60000 Time elasped:10.58 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:78.94 s\n",
      "Epoch [39/50], Training Loss: 19.60000 Time elasped:7.39 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.42 s\n",
      "Epoch [40/50], Training Loss: 19.60000 Time elasped:7.36 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:72.38 s\n",
      "Epoch [41/50], Training Loss: 19.60000 Time elasped:7.40 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:73.94 s\n",
      "Epoch [42/50], Training Loss: 19.60000 Time elasped:11.71 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:85.65 s\n",
      "Epoch [43/50], Training Loss: 19.60000 Time elasped:11.11 s\n",
      "ANN Test Accuracy: 10.000\n",
      "SNN Test Accuracy: 10.000 Time elasped:81.53 s\n",
      "Epoch [44/50], Training Loss: 19.60000 Time elasped:11.12 s\n",
      "ANN Test Accuracy: 10.000\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.backends.mps import is_available\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "num_epochs = 50\n",
    "\n",
    "ANN = Network_ANN()\n",
    "SNN = Network_SNN(time_window=40, threshold=1.0, max_rate=400)\n",
    "ANN.to(device)\n",
    "SNN.to(device)\n",
    "# optimizer = torch.optim.Adam(ANN.parameters(), lr=1, weight_decay=1e-5, betas=(0.9, 0.999))\n",
    "optimizer = torch.optim.Adam(ANN.parameters(), lr=0.01,  betas=(0.9, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 0.95**epoch)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "def cifar10_transform():\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_data_loaders(train_batch_size):\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./Dataset/cifar10\", train=True, download=True, transform=cifar10_transform())\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "\n",
    "    val_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./Dataset/cifar10\", train=False, download=False, transform=cifar10_transform())\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=128, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(train_loader, ANN, optimizer, criterion, device):\n",
    "    ANN.train()\n",
    "    running_loss = 0\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        batch_sz = inputs.size(0)\n",
    "        ANN.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.float().to(device)\n",
    "        labels_ = torch.zeros(batch_sz, 10).scatter_(\n",
    "            1, targets.view(-1, 1), 1).to(device)\n",
    "        outputs = ANN(inputs)\n",
    "        loss = criterion(outputs, labels_)\n",
    "        running_loss += loss.cpu().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return running_loss\n",
    "\n",
    "def test_model(loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "            batch_sz = inputs.size(0)\n",
    "            inputs = inputs.float().to(device)\n",
    "            labels_ = torch.zeros(batch_sz, 10).scatter_(\n",
    "                1, targets.view(-1, 1), 1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = criterion(outputs, labels_)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += float(targets.size(0))\n",
    "            correct += float(predicted.eq(targets).sum().cpu().item())\n",
    "    return correct / total\n",
    "\n",
    "train_loader, val_loader = get_data_loaders(train_batch_size=256)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    loss = train_model(train_loader, ANN, optimizer, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    print('Epoch [%d/%d], Training Loss: %.5f Time elasped:%.2f s' \n",
    "          % (epoch+1, num_epochs, loss, time.time()-start_time))\n",
    "    \n",
    "    acc_ann = test_model(val_loader, ANN, criterion, device)\n",
    "    print(\"ANN Test Accuracy: %.3f\" % (100 * acc_ann))\n",
    "    \n",
    "    SNN.load_state_dict(ANN.state_dict())\n",
    "    acc_snn = test_model(val_loader, SNN, criterion, device)\n",
    "    print(\"SNN Test Accuracy: %.3f Time elasped:%.2f s\" %\n",
    "          (100 * acc_snn, time.time()-start_time))\n",
    "\n",
    "ANN.normalize_nn(train_loader)\n",
    "for i, value in enumerate(ANN.factor_log):\n",
    "    print('Normalization Factor for Layer %d: %3.5f' % (i, value))\n",
    "\n",
    "# time_window: This parameter typically refers to the length of the time period over which spikes are integrated or observed. \n",
    "# It's a fundamental aspect of SNNs because unlike traditional artificial neurons that output a continuous value, spiking neurons fire binary spikes over time. \n",
    "# The time window parameter is used to define the time frame for processing these spikes.\n",
    "# \n",
    "# max_rate: In the context of SNNs, this parameter generally refers to the maximum firing rate of a neuron, which is the highest number of spikes that a neuron can emit in a given unit of time.\n",
    "# It's a way to constrain the activity of the neurons in the network, which can be important for controlling the dynamics of the network and preventing runaway feedback loops.\n",
    "\n",
    "SNN_normalized = Network_SNN(\n",
    "    time_window=50, threshold=1.2, max_rate=400)\n",
    "SNN_normalized.to(device)\n",
    "SNN_normalized.load_state_dict(ANN.state_dict())\n",
    "\n",
    "acc_snn_normalized = test_model(val_loader, SNN_normalized, criterion, device)\n",
    "print(\"Normalized SNN Test Accuracy: %.3f\" % (100 * acc_snn_normalized))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
