{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from spikingjelly.clock_driven.neuron import MultiStepLIFNode # MultiStepLIFNode: 다단계 LIF 뉴런\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_ # to_2tuple: 2-tuple로 변환하는 함수\n",
    "from timm.models.registry import register_model # register_model: 모델을 등록하는 함수\n",
    "from timm.models.vision_transformer import _cfg, Mlp, PatchEmbed, _create_vision_transformer # _cfg: 모델의 기본 설정을 반환하는 함수\n",
    "import torch.nn.functional as F\n",
    "from functools import partial # partial: 함수의 인자를 고정시키는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spike_Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, f\"dim {dim} should be divided by num_heads {num_heads}.\"\n",
    "        self.dim = dim # dim 저장\n",
    "        self.num_heads = num_heads # num_heads 저장\n",
    "        self.scale = 0.125 # scale 저장\n",
    "        self.q_linear = nn.Linear(dim, dim) # q_linear: query를 위한 fully-connected layer\n",
    "        self.q_bn = nn.BatchNorm1d(dim) # q_bn: query를 위한 batch normalization\n",
    "        self.q_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='cupy') # q_lif: query를 위한 LIF 뉴런\n",
    "\n",
    "        self.k_linear = nn.Linear(dim, dim) # k_linear: key를 위한 fully-connected layer\n",
    "        self.k_bn = nn.BatchNorm1d(dim) # k_bn: key를 위한 batch normalization\n",
    "        self.k_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='cupy') # k_lif: key를 위한 LIF 뉴런\n",
    "\n",
    "        self.v_linear = nn.Linear(dim, dim) # v_linear: value를 위한 fully-connected layer\n",
    "        self.v_bn = nn.BatchNorm1d(dim) # v_bn: value를 위한 batch normalization\n",
    "        self.v_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='cupy') # v_lif: value를 위한 LIF 뉴런\n",
    "        self.attn_lif = MultiStepLIFNode(tau=2.0, v_threshold=0.5, detach_reset=True, backend='cupy') # attn_lif: attention을 위한 LIF 뉴런\n",
    "\n",
    "        self.proj_linear = nn.Linear(dim, dim) # proj_linear: projection을 위한 fully-connected layer\n",
    "        self.proj_bn = nn.BatchNorm1d(dim) # proj_bn: projection을 위한 batch normalization\n",
    "        self.proj_lif = MultiStepLIFNode(tau=2.0, detach_reset=True, backend='cupy') # proj_lif: projection을 위한 LIF 뉴런\n",
    "\n",
    "    def forward(self, x): \n",
    "        T,B,N,C = x.shape\n",
    "\n",
    "        x_for_qkv = x.flatten(0, 1)  # x_for_qkv의 shape: (T * B, N, C)\n",
    "        q_linear_out = self.q_linear(x_for_qkv) # q_linear_out의 shape: (T * B, N, C)\n",
    "        q_linear_out = self.q_bn(q_linear_out. transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, C).contiguous() # q_linear_out의 shape: (T, B, N, C)\n",
    "        q_linear_out = self.q_lif(q_linear_out) # q_linear_out의 shape: (T, B, N, C)\n",
    "        q = q_linear_out.reshape(T, B, N, self.num_heads, C//self.num_heads).permute(0, 1, 3, 2, 4).contiguous() # q의 shape: (T, B, num_heads, N, C//num_heads)\n",
    "\n",
    "        k_linear_out = self.k_linear(x_for_qkv) # k_linear_out의 shape: (T * B, N, C)\n",
    "        k_linear_out = self.k_bn(k_linear_out. transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, C).contiguous() # k_linear_out의 shape: (T, B, N, C)\n",
    "        k_linear_out = self.k_lif(k_linear_out) # k_linear_out의 shape: (T, B, N, C)\n",
    "        k = k_linear_out.reshape(T, B, N, self.num_heads, C//self.num_heads).permute(0, 1, 3, 2, 4).contiguous() # k의 shape: (T, B, num_heads, N, C//num_heads)\n",
    "\n",
    "        v_linear_out = self.v_linear(x_for_qkv) # v_linear_out의 shape: (T * B, N, C)\n",
    "        v_linear_out = self.v_bn(v_linear_out. transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, C).contiguous() # v_linear_out의 shape: (T, B, N, C)\n",
    "        v_linear_out = self.v_lif(v_linear_out) # v_linear_out의 shape: (T, B, N, C)\n",
    "        v = v_linear_out.reshape(T, B, N, self.num_heads, C//self.num_heads).permute(0, 1, 3, 2, 4).contiguous() # v의 shape: (T, B, num_heads, N, C//num_heads)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale # attn의 shape: (T, B, num_heads, N, N)\n",
    "        x = attn @ v # x의 shape: (T, B, num_heads, N, C//num_heads)\n",
    "        x = x.transpose(2, 3).reshape(T, B, N, C).contiguous() # x의 shape: (T, B, N, num_heads * C//num_heads)\n",
    "        x = self.attn_lif(x) # x의 shape: (T, B, N, num_heads * C//num_heads)\n",
    "        x = x.flatten(0, 1) # x의 shape: (T * B, N, num_heads * C//num_heads)\n",
    "        x = self.proj_lif(self.proj_bn(self.proj_linear(x).transpose(-1, -2)).transpose(-1, -2).reshape(T, B, N, C)) # x의 shape: (T, B, N, C)\n",
    "        return x # x의 shape: (T, B, N, C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
